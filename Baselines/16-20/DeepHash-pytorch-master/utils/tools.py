import numpy as np
import torch.utils.data as util_data
from torchvision import transforms
import torch
import os
import re
from PIL import Image
from tqdm import tqdm
import torchvision.datasets as dsets

"""
æ ¹æ®æ•°æ®é›†åç§°è®¾ç½®åˆ†ç±»æ•°ã€topK è¯„ä»·èŒƒå›´å’Œæ•°æ®è·¯å¾„ç­‰ä¿¡æ¯ã€‚
"""
def config_dataset(config):
    base_path = "./data/"  # ç›¸å¯¹è·¯å¾„ï¼ŒåŸºäºé¡¹ç›®æ ¹ç›®å½•

    if "cifar" in config["dataset"]:
        config["topK"] = -1
        config["n_class"] = 10
    elif config["dataset"] in ["nuswide_21", "nuswide_21_m"]:
        config["topK"] = 5000
        config["n_class"] = 21
    elif config["dataset"] == "nuswide_81_m":
        config["topK"] = 5000
        config["n_class"] = 81
    elif config["dataset"] == "coco":
        config["topK"] = 5000
        config["n_class"] = 80
    elif config["dataset"] == "imagenet":
        config["topK"] = 1000
        config["n_class"] = 100
    elif config["dataset"] == "mirflickr":
        config["topK"] = 5000
        config["n_class"] = 38
    elif config["dataset"] in ["voc2012", "newvoc"]:
        config["topK"] = 1000
        config["n_class"] = 20

    config["data_path"] = base_path + config["dataset"] + "/"

    config["data"] = {
        "train_set": {"list_path": base_path + config["dataset"] + "/train.txt", "batch_size": config["batch_size"]},
        "database": {"list_path": base_path + config["dataset"] + "/database.txt", "batch_size": config["batch_size"]},
        "test": {"list_path": base_path + config["dataset"] + "/test.txt", "batch_size": config["batch_size"]}
    }

    return config



# é¢„å®šä¹‰çš„æ£€ç´¢æ ·æœ¬æ•°é‡èŒƒå›´ï¼Œç”¨äºç»˜åˆ¶ PR æ›²çº¿
draw_range = [1, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500,
              9000, 9500, 10000]

"""
æ ¹æ®æŸ¥è¯¢é›†ä¸æ•°æ®åº“çš„ç‰¹å¾å’Œæ ‡ç­¾è®¡ç®— Precision-Recall æ›²çº¿æ•°æ®ã€‚
"""
def pr_curve(rF, qF, rL, qL, draw_range=draw_range):
    #  https://blog.csdn.net/HackerTom/article/details/89425729
    n_query = qF.shape[0]
    Gnd = (np.dot(qL, rL.transpose()) > 0).astype(np.float32)
    Rank = np.argsort(CalcHammingDist(qF, rF))
    P, R = [], []
    for k in tqdm(draw_range):
        p = np.zeros(n_query)
        r = np.zeros(n_query)
        for it in range(n_query):
            gnd = Gnd[it]
            gnd_all = np.sum(gnd)
            if gnd_all == 0:
                continue
            asc_id = Rank[it][:k]
            gnd = gnd[asc_id]
            gnd_r = np.sum(gnd)
            p[it] = gnd_r / k
            r[it] = gnd_r / gnd_all
        P.append(np.mean(p))
        R.append(np.mean(r))
    return P, R


"""
è‡ªå®šä¹‰å›¾åƒæ•°æ®é›†ç±»ï¼Œä» txt åˆ—è¡¨ä¸­è¯»å–å›¾åƒè·¯å¾„ä¸å¤šæ ‡ç­¾æ•°æ®ã€‚
"""
class ImageList(object):

    def __init__(self, data_path, image_list, transform):
        self.imgs = [(data_path + val.split()[0], np.array([int(la) for la in val.split()[1:]])) for val in image_list]
        self.transform = transform

    def __getitem__(self, index):
        path, target = self.imgs[index]
        img = Image.open(path).convert('RGB')
        img = self.transform(img)
        # è¿”å›å¤„ç†åçš„å›¾åƒ tensorï¼Œæ ‡ç­¾ï¼ˆå‘é‡ï¼‰ï¼Œæ ·æœ¬ç´¢å¼•ï¼ˆæœ‰æ—¶ç”¨äºè¿½è¸ªæˆ–é‡‡æ ·ï¼‰
        return img, target, index

    def __len__(self):
        return len(self.imgs)

"""
å®šä¹‰å›¾åƒé¢„å¤„ç†æ“ä½œï¼ŒåŒ…æ‹¬ Resizeã€Cropã€ToTensor å’Œ Normalizeã€‚
æ ¹æ®æ˜¯ train_set è¿˜æ˜¯ test/database è®¾ç½®ä¸åŒçš„å˜æ¢ã€‚
"""
def image_transform(resize_size, crop_size, data_set):
    if data_set == "train_set":
        step = [transforms.RandomHorizontalFlip(), transforms.RandomCrop(crop_size)]
    else:
        step = [transforms.CenterCrop(crop_size)]
    return transforms.Compose([transforms.Resize(resize_size)]
                              + step +
                              [transforms.ToTensor(),
                               transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                                    std=[0.229, 0.224, 0.225])
                               ])

"""
CIFAR10 æ•°æ®é›†çš„è‡ªå®šä¹‰ç‰ˆæœ¬ï¼Œå°†æ ‡ç­¾è½¬ä¸º one-hotï¼Œå¹¶è¿›è¡Œ transformã€‚
"""
class MyCIFAR10(dsets.CIFAR10):
    def __getitem__(self, index):
        img, target = self.data[index], self.targets[index]
        img = Image.fromarray(img)
        img = self.transform(img)
        target = np.eye(10, dtype=np.int8)[np.array(target)]
        return img, target, index

"""
å¤„ç† CIFAR10 ç±»æ•°æ®é›†ï¼ŒæŒ‰ç…§å›ºå®šæ¯”ä¾‹åˆ’åˆ† trainã€test å’Œ database å¹¶è¿”å› DataLoaderã€‚
"""
def cifar_dataset(config):
    batch_size = config["batch_size"]

    train_size = 500
    test_size = 100

    if config["dataset"] == "cifar10-2":
        train_size = 5000
        test_size = 1000

    transform = transforms.Compose([
        transforms.Resize(config["crop_size"]),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    cifar_dataset_root = 'dataset/cifar/'
    # Dataset
    train_dataset = MyCIFAR10(root=cifar_dataset_root,
                              train=True,
                              transform=transform,
                              download=True)

    test_dataset = MyCIFAR10(root=cifar_dataset_root,
                             train=False,
                             transform=transform)

    database_dataset = MyCIFAR10(root=cifar_dataset_root,
                                 train=False,
                                 transform=transform)

    X = np.concatenate((train_dataset.data, test_dataset.data))
    L = np.concatenate((np.array(train_dataset.targets), np.array(test_dataset.targets)))

    first = True
    for label in range(10):
        index = np.where(L == label)[0]

        N = index.shape[0]
        perm = np.random.permutation(N)
        index = index[perm]

        if first:
            test_index = index[:test_size]
            train_index = index[test_size: train_size + test_size]
            database_index = index[train_size + test_size:]
        else:
            test_index = np.concatenate((test_index, index[:test_size]))
            train_index = np.concatenate((train_index, index[test_size: train_size + test_size]))
            database_index = np.concatenate((database_index, index[train_size + test_size:]))
        first = False

    if config["dataset"] == "cifar10":
        # test:1000, train:5000, database:54000
        pass
    elif config["dataset"] == "cifar10-1":
        # test:1000, train:5000, database:59000
        database_index = np.concatenate((train_index, database_index))
    elif config["dataset"] == "cifar10-2":
        # test:10000, train:50000, database:50000
        database_index = train_index

    train_dataset.data = X[train_index]
    train_dataset.targets = L[train_index]
    test_dataset.data = X[test_index]
    test_dataset.targets = L[test_index]
    database_dataset.data = X[database_index]
    database_dataset.targets = L[database_index]

    print("train_dataset", train_dataset.data.shape[0])
    print("test_dataset", test_dataset.data.shape[0])
    print("database_dataset", database_dataset.data.shape[0])

    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=batch_size,
                                               shuffle=True,
                                               num_workers=4)

    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                              batch_size=batch_size,
                                              shuffle=False,
                                              num_workers=4)

    database_loader = torch.utils.data.DataLoader(dataset=database_dataset,
                                                  batch_size=batch_size,
                                                  shuffle=False,
                                                  num_workers=4)

    return train_loader, test_loader, database_loader, \
           train_index.shape[0], test_index.shape[0], database_index.shape[0]

"""
æ ¹æ® config é…ç½®ï¼ŒåŠ è½½é CIFAR çš„é€šç”¨å›¾åƒæ•°æ®é›†å¹¶è¿”å›å¯¹åº”çš„ DataLoaderã€‚
"""
def get_data(config):
    if "cifar" in config["dataset"]:
        return cifar_dataset(config)

    dsets = {}
    dset_loaders = {}
    data_config = config["data"]

    for data_set in ["train_set", "test", "database"]:
        dsets[data_set] = ImageList(config["data_path"],
                                    open(data_config[data_set]["list_path"]).readlines(),
                                    transform=image_transform(config["resize_size"], config["crop_size"], data_set))
        print(data_set, len(dsets[data_set]))
        dset_loaders[data_set] = util_data.DataLoader(dsets[data_set],
                                                      batch_size=data_config[data_set]["batch_size"],
                                                      shuffle=True, num_workers=4)

    return dset_loaders["train_set"], dset_loaders["test"], dset_loaders["database"], \
           len(dsets["train_set"]), len(dsets["test"]), len(dsets["database"])



"""
æ¸…ç† save ç›®å½•ä¸­é™¤æœ€ä½³ mAP æ–‡ä»¶å¤–çš„æ‰€æœ‰æ¨¡å‹/ä¸­é—´æ–‡ä»¶
"""
def clean_save_dir_keep_best(save_dir, dataset_name):
    if not os.path.exists(save_dir):
        print(f"[clean_save_dir_keep_best] âš ï¸ ç›®å½•ä¸å­˜åœ¨: {save_dir}ï¼Œè·³è¿‡æ¸…ç†ã€‚")
        return

    # æ”¯æŒ dataset_tag-<10ä½æµ®ç‚¹æ•°>-xxx
    pattern = re.compile(r"^(.+)-(\d+\.\d{10})-")
    file_groups = {}  # {score: [file1, file2, ...]}

    for filename in os.listdir(save_dir):
        match = pattern.match(filename)
        if match:
            score = float(match.group(2))
            file_path = os.path.join(save_dir, filename)
            file_groups.setdefault(score, []).append(file_path)

    if not file_groups:
        print("[clean_save_dir_keep_best] âŒ No matching files to clean.")
        return

    best_score = max(file_groups.keys())
    print(f"[clean_save_dir_keep_best] âœ… Keep mAP={best_score:.10f}, delete other {len(file_groups) - 1} groups.")

    deleted = 0
    for score, files in file_groups.items():
        if score != best_score:
            for file_path in files:
                try:
                    os.remove(file_path)
                    deleted += 1
                except Exception as e:
                    print(f"Failed to delete {file_path}: {e}")

    print(f"[clean_save_dir_keep_best] ğŸ§¹ Deleted {deleted} files.")


"""
é€šè¿‡æ¨¡å‹æå–å›¾åƒçš„å“ˆå¸Œç‰¹å¾å‘é‡ï¼ˆsignï¼‰ä¸å¯¹åº”æ ‡ç­¾ï¼Œå¹¶è¿”å›å…¨éƒ¨ç»“æœã€‚
"""
def compute_result(dataloader, net, device):
    bs, clses = [], []
    net.eval()
    for img, cls, _ in tqdm(dataloader):
        clses.append(cls)
        bs.append((net(img.to(device))).data.cpu())
    return torch.cat(bs).sign(), torch.cat(clses)

"""
è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œç çŸ©é˜µä¹‹é—´çš„æ±‰æ˜è·ç¦»ã€‚
"""
def CalcHammingDist(B1, B2):
    q = B2.shape[1]
    distH = 0.5 * (q - np.dot(B1, B2.transpose()))
    return distH

"""
è®¡ç®— mAPï¼ˆmean Average Precisionï¼‰æŒ‡æ ‡ï¼Œè¡¡é‡å“ˆå¸Œæ£€ç´¢è´¨é‡ã€‚
rB	retrieval hash codeï¼ˆæ•°æ®åº“å›¾åƒçš„å“ˆå¸Œç ï¼‰ï¼Œshape: [N_database, n_bits]
qB	query hash codeï¼ˆæŸ¥è¯¢å›¾åƒçš„å“ˆå¸Œç ï¼‰ï¼Œshape: [N_query, n_bits]
retrievalL	æ•°æ®åº“å›¾åƒçš„æ ‡ç­¾ï¼ˆmulti-hotï¼‰ï¼Œshape: [N_database, n_class]
queryL	æŸ¥è¯¢å›¾åƒçš„æ ‡ç­¾ï¼Œshape: [N_query, n_class]
topk	æŒ‡å®šä»æ•°æ®åº“ä¸­æ£€ç´¢çš„å‰ topk ä¸ªæ ·æœ¬ç”¨äºè¯„ä¼°

è¿”å›çš„æ˜¯ä¸€ä¸ªæµ®ç‚¹æ•° topkmapï¼Œè¡¨ç¤ºï¼š
åœ¨å‰ topk ä¸ªæ£€ç´¢ç»“æœä¸­ï¼Œå¹³å‡æ¯ä¸ªæŸ¥è¯¢çš„æ£€ç´¢ç²¾åº¦å‡å€¼ï¼ˆmean Average Precisionï¼‰
å³ï¼šæ•´ä½“ç³»ç»Ÿçš„å¹³å‡æ£€ç´¢æ€§èƒ½æŒ‡æ ‡ï¼ˆè¶Šé«˜è¶Šå¥½ï¼Œæœ€å¤§ä¸º 1.0ï¼‰


ä»¥æ¯ä¸ªæŸ¥è¯¢ä¸ºä¾‹ï¼š
è®¡ç®—æŸ¥è¯¢æ ·æœ¬ä¸æ‰€æœ‰æ•°æ®åº“æ ·æœ¬çš„æ ‡ç­¾æ˜¯å¦æœ‰é‡åˆï¼ˆground truthï¼‰ï¼›
è®¡ç®—å®ƒä¸æ‰€æœ‰æ•°æ®åº“æ ·æœ¬çš„æ±‰æ˜è·ç¦» CalcHammingDistï¼›
æ’åºåï¼Œå–å‰ topk ä¸ªæ’åºç»“æœï¼›
è®¡ç®—è¿™äº›ç»“æœä¸­ relevant çš„å¹³å‡ç²¾åº¦ï¼›
å¯¹æ‰€æœ‰æŸ¥è¯¢æ ·æœ¬åšå¹³å‡ï¼Œå¾—åˆ°æœ€ç»ˆ topkmapã€‚
"""
def CalcTopMap(rB, qB, retrievalL, queryL, topk):
    num_query = queryL.shape[0]
    topkmap = 0
    for iter in tqdm(range(num_query)):
        gnd = (np.dot(queryL[iter, :], retrievalL.transpose()) > 0).astype(np.float32)
        hamm = CalcHammingDist(qB[iter, :], rB)
        ind = np.argsort(hamm)
        gnd = gnd[ind]

        tgnd = gnd[0:topk]
        tsum = np.sum(tgnd).astype(int)
        if tsum == 0:
            continue
        count = np.linspace(1, tsum, tsum)

        tindex = np.asarray(np.where(tgnd == 1)) + 1.0
        topkmap_ = np.mean(count / (tindex))
        topkmap = topkmap + topkmap_
    topkmap = topkmap / num_query
    return topkmap
